## 分区容错性

### 具备分区容忍性的强一致性算法
- 推导
    1. 当出现网络分区后，要防止出现一致性问题，从 CAP 定理可知，只能放弃可用性。
    2. 当网络发生分区后，各分区内都可以独立运转，并且都会继续处理外部请求。
    3. 为了尽可能地保留整体可用性，两害相权取其轻，应该放弃掉节点数较少的分区。
    4. 每个分区都不知道其他分区的情况，为了保证整体最大可用性，获胜的分区需要多于整体一半的节点数。
    5. 同时，从解决数据冲突的角度看，只有当至少有一个节点能同时接收到相互冲突的数据时，才有可能发现冲突。数学告诉我们，必须覆盖超过一半的节点。
- 特点
    - 节点总数必须是奇数 n。 
    - 数据成功发送给至少 (n/2 + 1) 个节点后，认为提交成功。 
    - 发生网络分区后，节点数多余 (n/2 + 1) 的分区（可以没有，则整个系统暂停）继续工作，其他分区暂停服务。

### 共识算法
- 使用场景
    - leader 选举
    - 分布式锁
    - 成员判活
    - 原子广播
- Paxos
    - 核心角色：
        - Proposer，发起提议
        - Acceptor，对提议投票
    - 阶段：
        - Phase 1:决定了谁的提议会被接受，而不确定具体提议是什么
            - **Prepare 阶段**，proposer 向 acceptors 发起 proposal(n, v)，n 为全局唯一并递增的整数。
            - **Promise 阶段**，acceptor 接收到 proposal 后，检查发现 n > 自己之前接受过的 proposal 编号，则接受当前 proposal 并在回复里带上上一次接受的 proposal 的信息，同时承诺不会再接受 < n 的任何 proposal。否则可以忽略这个请求。
        - Phase 2:正式提议
            - **Accept 阶段**，proposer 在接收到超过半数的肯定回复后，向 acceptors 发送请求 accept(n, v)，其中 v 为准备阶段收到的回复中编号最大的值，如果没有，则使用自己第一阶段提议的值。
            - **Accepted 阶段**，acceptor 接受请求后，检查 n，在不违背第一阶段承诺的前提，接受提交过来的新值。    
        - **决定谁的提议会最终达成共识的，不是谁的提议编号更大，而是谁的提议更快被超过半数的节点接受。**
    - Paxos vs 2PC
        - Paxos 只需要*超过半数的节点*表决，而 2PC 需要*所有节点*表决。而网络分区发生时，是一定凑不齐所有节点的。
        - Paxos 支持有*多个 proposer*，而 2PC 只能有*一个 Coordinator*。一旦发生关键节点宕机，Paxos 可以正常运转，而 2PC 由于存在单点故障就阻塞了。
- Raft
    - 节点状态：
        - Leader，只有 leader 能发起 proposal，leader 故障后自动选举新的 leader
        - Candidate，当follower在超时时间内没有收到leader的心跳就会发起选举，变为candidate
        - Follower，节点的初始状态，参与投票
    - 阶段：
        - leader election：
            - 任何节点如果在心跳时间内没有收到 leader 发过来的新的心跳，就可以发起投票，得到超过半数以上投票的节点变为leader。
            - 活锁问题：每个 Candidate在发起新提议前都随机等待一小段时间。随机性保证了同时发起投票的可能性足够低。
        - log replication：
            - 选举出 leader 后，就可以对外提供数据读写服务。
            - Term，或者在其他系统里叫 Epoch，是全局递增的整数，表示 leader 的任期。每轮新的选举都会导致 Term 加 1  。 
            - **Term + 超过半数投票就很好的解决了网络分区下的数据一致性问题。**
        