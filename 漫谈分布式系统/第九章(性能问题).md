## 性能问题

### MapReduce 的性能问题
IO操作
- map 阶段，读取文件数据给 map function 处理，处理结果写到一个环形缓存，达到阈值后会以分区为单位拆分、排序，再 spill to disk。_这里涉及磁盘 IO_。
- map 阶段的每次 spill 操作都会产生一个文件，所以最后会通过几轮的 merge，来保证最后每个分区只有一个排序好的文件。_这里涉及磁盘 IO_。
- shuffle 阶段，需要从各个 mapper 把对应数据 copy 到对应 reducer。_这里涉及网络 IO_。
- 如果从 mapper 读到的数据很小，就会先放到缓存，达到阈值后再 merge 然后 spill to disk；如果从 mapper 读到的数据很大，就会直接存到磁盘。_这里涉及磁盘 IO_。
- reducer 从 mapper copy 数据的同时，会有独立的线程持续对拿到的数据做 merge。最后一轮 merge 做完后，把结果交给 reduce function 处理。_这里涉及磁盘 IO_。

### 基于内存计算的Spark
#### cache(把数据缓存在内存上)
- Spark 的核心是 RDD（Resilient Distributed Dataset），所有操作都围绕这个基于内存的分布式数据结构展开。
- StorageLevel：
  1. MEMORY_ONLY，全放内存。
  2. MEMORY_AND_DISK，优先放内存，放不下的放硬盘。
  3. MEMORY_ONLY_SER，序列化后放内存。**(序列化是为了减少内存开销)**
  4. MEMORY_AND_DISK_SER，序列化后优先放内存，其次硬盘。
  5. DISK_ONLY，全放硬盘。
  6. MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.，以上设置的二副本形式。
  7. OFF_HEAP (experimental)，放堆外内存。
- cache 一般在两种场景下用得比较多(多次访问同样的数据)
  - 交互式的数据处理。
  - 诸如机器学习和深度学习的迭代算法。
  
#### pipelining
- Spark 以 shuffle 操作为间隔，把一个 app 拆分成很多 stage。在每个 stage 内，对数据的处理都以 pipeline 的形式进行。
- 比如 `ds.map().filter().map()` ，ds 里的每一行数据，都会连续做完 map、filter 和 map 处理，然后才会把数据写到内存或硬盘，而不用像 MR 那样起两个任务去跑，每个任务跑完都需要把数据写到硬盘。

### 存储代理的Alluxio
- Alluxio 想做的是**基于内存的统一存储系统**，上面承接各种分布式计算框架，下面对接五花八门的存储后端。
- 统一的文件抽象层：Namespace 方案(通过挂载目录的方式对接不同的存储后端)
- 多层缓存：提供异构的多层缓存
  - 少量的热数据保存在自己的 worker 上，以提供高性能，大量的冷数据仍然保存在 UFS 上，Alluxio 会把请求透传下去。
  - 淘汰策略有：
    1. LRUEvictor，最常见的 Least-Recently-Used 算法。
    2. GreedyEvictor，贪婪算法，随机淘汰，直至满足需要的空间。
    3. LRFUEvictor，Least-Recently-Used 和 Least-Frequently-Used 结合，可以设置二者的权重。
    4. PartialLRUEvictor，只对剩余空间最多的挂载目录做 LRU。
- 数据一致性
  - 提供了 checkConsistency 命令来对指定目录做一致性检测
  - 所有读写请求都通过 Alluxio，不直接写 UFS。
    - 读请求
        1. Local Cache Hit，在本地 worker 命中缓存，并通过短路读（short-circuit ）直接访问内存和硬盘，不用 worker 进程倒一手。**是性能最高的读方式**。
        2. Remote Cache Hit，在远端 worker 命中缓存，通过远端 worker 拿到数据后，会同时让本地 worker 也保存一份，以为后续可能的请求提供 local cache。
        3. Cache Miss，没有缓存能命中，也就是 Alluxio 中没有这份数据，那就只能通过 worker 找 UFS 取了，同样，取完也会在本地 worker 缓存一份。
        4. Cache Skip，跳过 Alluxio 的缓存，直接访问 UFS。
    - 写请求
        1. Must Cache，数据会写到本地 worker，但不会往 UFS 写。
        2. Cache Through，数据除了写到本地 worker，还会同步写给 UFS。
        3. Aysnc Through，数据除了写到本地 worker，会异步写给 UFS。
        4. Through，数据直接写给 UFS，不在本地 worker 缓存。
  - 可以为文件和目录设置 TTL，自动淘汰，一来腾空间，二来降低不一致的可能性。
- Alluxio 提升性能的前提是基于两个认识：
    - 本地性能上 MEM > SSD > HDD
    - 网络性能上 client -> worker > client -> UFS    
  
